# ü©∫ TinyLlama Nurse: Sistema de Entrenamiento y Fusi√≥n de IA M√©dica

<p align="center">
  <img src="https://img.shields.io/badge/Model-TinyLlama--1.1B--Chat-orange?style=for-the-badge&logo=huggingface" alt="Model">
  <img src="https://img.shields.io/badge/Optimization-LoRA%20(PEFT)-blue?style=for-the-badge" alt="Optimization">
  <img src="https://img.shields.io/badge/Domain-Enfermer√≠a%20y%20Salud-red?style=for-the-badge" alt="Domain">
</p>

Este repositorio contiene un entorno completo para el refinamiento local del modelo **TinyLlama-1.1B** mediante t√©cnicas de **Fine-Tuning con LoRA**, orientado espec√≠ficamente a la creaci√≥n de un asistente virtual para enfermer√≠a universitaria.

---

## üöÄ Flujo de Trabajo y Scripts detallados

A continuaci√≥n, se detalla la l√≥gica interna de cada componente del sistema en su orden de ejecuci√≥n:

### üß† 1. Entrenamiento: `01_refinamiento.py`
Este script es el coraz√≥n del proceso de aprendizaje. Su funci√≥n es "inyectar" conocimiento espec√≠fico sin alterar dr√°sticamente la estructura del modelo original.

*   **Logic & Hardware**:
    *   Detecta autom√°ticamente si cuentas con una **GPU NVIDIA (CUDA)**. Si se detecta, activa la precisi√≥n de punto flotante de 16 bits (`fp16=True`), lo que duplica la velocidad y reduce el consumo de VRAM.
    *   Carga el modelo base en formato de 16 bits (`torch.float16`) para optimizar recursos en arquitecturas como la RTX 2060.
*   **T√©cnica LoRA (PEFT)**:
    *   Configura una matriz de bajo rango (`r=8`, `lora_alpha=16`) que se enfoca exclusivamente en las capas de atenci√≥n `q_proj` y `v_proj`. Esto permite que solo se entrenen unos pocos millones de par√°metros, ahorrando gigabytes de memoria.
*   **Procesamiento de Datos**:
    *   Lee el archivo `primeros_auxilios_500.jsonl` y formatea cada entrada como un di√°logo `Usuario / Asistente`.
    *   Aplica un l√≠mite de **256 tokens** por secuencia para mantener la eficiencia.

### üîó 2. Fusi√≥n de Pesos: `02_create_model_nurse.py`
Cuando el entrenamiento termina, los cambios (adaptadores) viven en una carpeta separada. Este script se encarga de integrarlos permanentemente al modelo base.

*   **Proceso de Merge**:
    *   Carga el modelo base limpio en un espacio de memoria de 32 bits (`float32`) para garantizar la m√°xima fidelidad durante la fusi√≥n.
    *   Carga los adaptadores aprendidos desde la carpeta `./modelo_chat`.
    *   Ejecuta el comando `model.merge_and_unload()`, que realiza una suma matem√°tica de los pesos de los adaptadores sobre los pesos originales.
*   **Resultado**: Crea un modelo "standalone" en `./model_nurse_final` que ya no requiere de la librer√≠a `PEFT` para funcionar, siendo mucho m√°s r√°pido en inferencia.

### üì¶ 3. Empaquetado y Exportaci√≥n: `03_export_model_nurse.py`
Debido a que los modelos de lenguaje pueden pesar varios gigabytes, un script de compresi√≥n est√°ndar podr√≠a fallar o tardar demasiado sin dar feedback.

*   **Compresi√≥n Inteligente**:
    *   **Modo Granular**: Primero escanea todos los archivos del modelo para calcular el peso total exacto.
    *   **Barra de Progreso (tqdm)**: Muestra en tiempo real cu√°ntos Gigabytes se han comprimido y a qu√© velocidad (MB/s).
    *   **Soporte Large File**: Utiliza el est√°ndar **Zip64** y lectura por bloques (chunks de 1MB) para manejar archivos de m√°s de 4GB sin saturar la memoria RAM del sistema.

---

## üè• Especializaci√≥n M√©dica: `medalpaca_training/`

Esta carpeta contiene una versi√≥n "Premium" de los scripts para investigadores que deseen llevar el modelo a un nivel de conocimiento m√©dico profesional.

### üì• Procesador: `descargar_medalpaca.py`
*   Conecta con el Hugging Face Hub para descargar el dataset `somosnlp/spanish_medica_llm`.
*   **Formateo Inteligente**: Clasifica autom√°ticamente si el dato es un caso cl√≠nico (clinic_case) o una pregunta m√©dica simple, asign√°ndole un "System Prompt" adecuado para guiar la respuesta de la IA.

### üå°Ô∏è Entrenador Maestro: `refinamiento_medalpaca.py`
*   Dise√±ado para procesar m√°s de **130,000 registros m√©dicos**.
*   **Optimizaciones Extremas**: 
    *   Activa `gradient_checkpointing_enable()`, lo que permite entrenar modelos grandes en GPUs con poca memoria a cambio de un ligero coste en velocidad de CPU.
    *   Usa un `lr_scheduler_type="cosine"`, que reduce la velocidad de aprendizaje de forma suave, permitiendo que el modelo aprenda detalles m√©dicos finos sin "olvidar" lo anterior.
    *   Aumenta la longitud de contexto a **512 tokens**.

---

## üõ†Ô∏è Requisitos del Sistema
*   **Sistema Operativo**: Windows 10/11 con PowerShell.
*   **Entorno**: Python 3.10 o superior (recomendado 3.11).
*   **Hardware**: 
    *   M√≠nimo: 16GB RAM + CPU.
    *   **Recomendado**: NVIDIA GPU con 6GB+ VRAM (ej. RTX 2060, 3060, 4060).
*   **Librer√≠as Cr√≠ticas**: `transformers`, `torch` (con soporte CUDA), `peft`, `datasets`, `tqdm`.

---

## üìÇ √Årbol de Archivos Importante

```text
E:\IA-UNIVERSIDAD\TINYLLAMA\
‚îú‚îÄ‚îÄ 01_refinamiento.py        <-- Entrenamiento de Enfermer√≠a (PASO 1)
‚îú‚îÄ‚îÄ 02_create_model_nurse.py  <-- Fusi√≥n de Modelo Final (PASO 2)
‚îú‚îÄ‚îÄ 03_export_model_nurse.py  <-- Compresor con barra de progreso (PASO 3)
‚îú‚îÄ‚îÄ requirements.txt          <-- Dependencias necesarias
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ PRACTICA_TINYLLAMA.md <-- Gu√≠a acad√©mica detallada
‚îî‚îÄ‚îÄ medalpaca_training/       <-- M√≥dulo avanzado de medicina masiva
    ‚îú‚îÄ‚îÄ descargar_medalpaca.py
    ‚îî‚îÄ‚îÄ refinamiento_medalpaca.py
```

---
<p align="center">
  <i>Desarrollado para la formaci√≥n acad√©mica en Inteligencia Artificial y Ciencias de la Salud.</i>
</p>

## Cr√©ditos y contacto
- Proyecto realizado por Jhoan Acosta - Blazkull.
- Para dudas o mejoras, abre un issue o contacta al autor.
